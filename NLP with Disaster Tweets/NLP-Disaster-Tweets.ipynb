{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Stuff\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import re\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the data\n",
    "train=pd.read_csv('train.csv',index_col='id')\n",
    "test=pd.read_csv('test.csv',index_col='id')\n",
    "y=train['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping these columns to focus only on the actual tweets\n",
    "train.drop(['location','keyword'],inplace=True,axis=1)\n",
    "test.drop(['location','keyword'],inplace=True,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "113461"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Extracting all the words from the tweets in the Training Data\n",
    "def create_corpus(train):\n",
    "    corpus=[]\n",
    "    num_words = 0\n",
    "\n",
    "    for i in range(len(train)):\n",
    "        for j in range(len(train.iloc[i,0].split())):\n",
    "            corpus.append(train.iloc[i,0].split()[j])\n",
    "    return corpus\n",
    "len(create_corpus(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 2575),\n",
       " ('a', 1845),\n",
       " ('to', 1805),\n",
       " ('in', 1757),\n",
       " ('of', 1722),\n",
       " ('and', 1302),\n",
       " ('I', 1197),\n",
       " ('for', 820),\n",
       " ('is', 814),\n",
       " ('on', 773)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating a Dict for counting the number of times each word occurs\n",
    "\n",
    "def word_count(corpus):\n",
    "    wcount=defaultdict(int)\n",
    "    for i in corpus:\n",
    "        wcount[i]+=1\n",
    "    return wcount\n",
    "        \n",
    "words = word_count(create_corpus(train))\n",
    "\n",
    "#Creating function to return 10 most used words\n",
    "def most_used(x):\n",
    "    return sorted(words.items(),key=lambda kv: kv[1],reverse=True)[:10]\n",
    "\n",
    "top=most_used(words)\n",
    "top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 10 artists>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Visualising the most used words in the training dataset\n",
    "a,b=zip(*top)\n",
    "plt.bar(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7503 7613\n"
     ]
    }
   ],
   "source": [
    "print(train['text'].nunique(),train['text'].count())  #Some tweets are identical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>Check these out: http://t.co/rOI2NSmEJJ http:/...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>Check these out: http://t.co/rOI2NSmEJJ http:/...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>320 [IR] ICEMOON [AFTERSHOCK] | http://t.co/vA...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>320 [IR] ICEMOON [AFTERSHOCK] | http://t.co/vA...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>320 [IR] ICEMOON [AFTERSHOCK] | http://t.co/TH...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10855</th>\n",
       "      <td>Evacuation order lifted for town of Roosevelt:...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10867</th>\n",
       "      <td>#stormchase Violent Record Breaking EF-5 El Re...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10870</th>\n",
       "      <td>@aria_ahrary @TheTawniest The out of control w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10871</th>\n",
       "      <td>M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10872</th>\n",
       "      <td>Police investigating after an e-bike collided ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>179 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  target\n",
       "id                                                              \n",
       "59     Check these out: http://t.co/rOI2NSmEJJ http:/...       0\n",
       "68     Check these out: http://t.co/rOI2NSmEJJ http:/...       0\n",
       "156    320 [IR] ICEMOON [AFTERSHOCK] | http://t.co/vA...       0\n",
       "165    320 [IR] ICEMOON [AFTERSHOCK] | http://t.co/vA...       0\n",
       "171    320 [IR] ICEMOON [AFTERSHOCK] | http://t.co/TH...       0\n",
       "...                                                  ...     ...\n",
       "10855  Evacuation order lifted for town of Roosevelt:...       1\n",
       "10867  #stormchase Violent Record Breaking EF-5 El Re...       1\n",
       "10870  @aria_ahrary @TheTawniest The out of control w...       1\n",
       "10871  M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...       1\n",
       "10872  Police investigating after an e-bike collided ...       1\n",
       "\n",
       "[179 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duplicate = train[train.duplicated('text',keep = False)] \n",
    "duplicate #Shows all tweets that are repeated atleast twice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>Check these out: http://t.co/rOI2NSmEJJ http:/...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>Check these out: http://t.co/rOI2NSmEJJ http:/...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>320 [IR] ICEMOON [AFTERSHOCK] | http://t.co/vA...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>320 [IR] ICEMOON [AFTERSHOCK] | http://t.co/vA...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>320 [IR] ICEMOON [AFTERSHOCK] | http://t.co/TH...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10855</th>\n",
       "      <td>Evacuation order lifted for town of Roosevelt:...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10867</th>\n",
       "      <td>#stormchase Violent Record Breaking EF-5 El Re...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10870</th>\n",
       "      <td>@aria_ahrary @TheTawniest The out of control w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10871</th>\n",
       "      <td>M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10872</th>\n",
       "      <td>Police investigating after an e-bike collided ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>157 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  target\n",
       "id                                                              \n",
       "59     Check these out: http://t.co/rOI2NSmEJJ http:/...       0\n",
       "68     Check these out: http://t.co/rOI2NSmEJJ http:/...       0\n",
       "156    320 [IR] ICEMOON [AFTERSHOCK] | http://t.co/vA...       0\n",
       "165    320 [IR] ICEMOON [AFTERSHOCK] | http://t.co/vA...       0\n",
       "171    320 [IR] ICEMOON [AFTERSHOCK] | http://t.co/TH...       0\n",
       "...                                                  ...     ...\n",
       "10855  Evacuation order lifted for town of Roosevelt:...       1\n",
       "10867  #stormchase Violent Record Breaking EF-5 El Re...       1\n",
       "10870  @aria_ahrary @TheTawniest The out of control w...       1\n",
       "10871  M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...       1\n",
       "10872  Police investigating after an e-bike collided ...       1\n",
       "\n",
       "[157 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duplicate2 = train[train.duplicated(keep=False)]\n",
    "duplicate2#Shows tweets that have same targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1723</th>\n",
       "      <td>Mmmmmm I'm burning.... I'm burning buildings I...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1752</th>\n",
       "      <td>I Pledge Allegiance To The P.O.P.E. And The Bu...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1760</th>\n",
       "      <td>like for the music video I want some real acti...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1922</th>\n",
       "      <td>Mmmmmm I'm burning.... I'm burning buildings I...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1950</th>\n",
       "      <td>like for the music video I want some real acti...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1968</th>\n",
       "      <td>I Pledge Allegiance To The P.O.P.E. And The Bu...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4076</th>\n",
       "      <td>.POTUS #StrategicPatience is a strategy for #G...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5662</th>\n",
       "      <td>Who is bringing the tornadoes and floods. Who ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6012</th>\n",
       "      <td>Caution: breathing may be hazardous to your he...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6017</th>\n",
       "      <td>Caution: breathing may be hazardous to your he...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6031</th>\n",
       "      <td>#foodscare #offers2go #NestleIndia slips into ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6112</th>\n",
       "      <td>Hellfire is surrounded by desires so be carefu...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6113</th>\n",
       "      <td>Hellfire! We donÛªt even want to think about ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6123</th>\n",
       "      <td>#Allah describes piling up #wealth thinking it...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6134</th>\n",
       "      <td>Hellfire! We donÛªt even want to think about ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6220</th>\n",
       "      <td>RT NotExplained: The only known image of infam...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6223</th>\n",
       "      <td>RT NotExplained: The only known image of infam...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6566</th>\n",
       "      <td>CLEARED:incident with injury:I-495  inner loop...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8018</th>\n",
       "      <td>wowo--=== 12000 Nigerian refugees repatriated ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8044</th>\n",
       "      <td>wowo--=== 12000 Nigerian refugees repatriated ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9470</th>\n",
       "      <td>In #islam saving a person is equal in reward t...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9472</th>\n",
       "      <td>In #islam saving a person is equal in reward t...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  target\n",
       "id                                                             \n",
       "1723  Mmmmmm I'm burning.... I'm burning buildings I...     1.0\n",
       "1752  I Pledge Allegiance To The P.O.P.E. And The Bu...     0.0\n",
       "1760  like for the music video I want some real acti...     1.0\n",
       "1922  Mmmmmm I'm burning.... I'm burning buildings I...     0.0\n",
       "1950  like for the music video I want some real acti...     0.0\n",
       "1968  I Pledge Allegiance To The P.O.P.E. And The Bu...     1.0\n",
       "4076  .POTUS #StrategicPatience is a strategy for #G...     0.0\n",
       "5662  Who is bringing the tornadoes and floods. Who ...     1.0\n",
       "6012  Caution: breathing may be hazardous to your he...     1.0\n",
       "6017  Caution: breathing may be hazardous to your he...     0.0\n",
       "6031  #foodscare #offers2go #NestleIndia slips into ...     0.0\n",
       "6112  Hellfire is surrounded by desires so be carefu...     1.0\n",
       "6113  Hellfire! We donÛªt even want to think about ...     0.0\n",
       "6123  #Allah describes piling up #wealth thinking it...     1.0\n",
       "6134  Hellfire! We donÛªt even want to think about ...     1.0\n",
       "6220  RT NotExplained: The only known image of infam...     0.0\n",
       "6223  RT NotExplained: The only known image of infam...     1.0\n",
       "6566  CLEARED:incident with injury:I-495  inner loop...     0.0\n",
       "8018  wowo--=== 12000 Nigerian refugees repatriated ...     1.0\n",
       "8044  wowo--=== 12000 Nigerian refugees repatriated ...     0.0\n",
       "9470  In #islam saving a person is equal in reward t...     0.0\n",
       "9472  In #islam saving a person is equal in reward t...     1.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#After comparing above two results we can say some tweets despite having same content they have multiple targets \n",
    "#this returns all the multiple target tweets\n",
    "duplicate[~duplicate.isin(duplicate2)].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We need to manually rectify these mistakes as we cannot do anything else\n",
    "\n",
    "train.loc[train['text'] == 'like for the music video I want some real action shit like burning buildings and police chases not some weak ben winston shit', 'target'] = 0\n",
    "train.loc[train['text'] == 'Hellfire is surrounded by desires so be careful and donÛªt let your desires control you! #Afterlife', 'target'] = 0\n",
    "train.loc[train['text'] == 'To fight bioterrorism sir.', 'target'] = 0\n",
    "train.loc[train['text'] == '.POTUS #StrategicPatience is a strategy for #Genocide; refugees; IDP Internally displaced people; horror; etc. https://t.co/rqWuoy1fm4', 'target'] = 1\n",
    "train.loc[train['text'] == 'CLEARED:incident with injury:I-495  inner loop Exit 31 - MD 97/Georgia Ave Silver Spring', 'target'] = 1\n",
    "train.loc[train['text'] == '#foodscare #offers2go #NestleIndia slips into loss after #Magginoodle #ban unsafe and hazardous for #humanconsumption', 'target'] = 0\n",
    "train.loc[train['text'] == 'In #islam saving a person is equal in reward to saving all humans! Islam is the opposite of terrorism!', 'target'] = 0\n",
    "train.loc[train['text'] == 'Who is bringing the tornadoes and floods. Who is bringing the climate change. God is after America He is plaguing her\\n \\n#FARRAKHAN #QUOTE', 'target'] = 1\n",
    "train.loc[train['text'] == 'RT NotExplained: The only known image of infamous hijacker D.B. Cooper. http://t.co/JlzK2HdeTG', 'target'] = 1\n",
    "train.loc[train['text'] == \"Mmmmmm I'm burning.... I'm burning buildings I'm building.... Oooooohhhh oooh ooh...\", 'target'] = 0\n",
    "train.loc[train['text'] == \"wowo--=== 12000 Nigerian refugees repatriated from Cameroon\", 'target'] = 0\n",
    "train.loc[train['text'] == \"He came to a land which was engulfed in tribal war and turned it into a land of peace i.e. Madinah. #ProphetMuhammad #islam\", 'target'] = 0\n",
    "train.loc[train['text'] == \"Hellfire! We donÛªt even want to think about it or mention it so letÛªs not do anything that leads to it #islam!\", 'target'] = 0\n",
    "train.loc[train['text'] == \"The Prophet (peace be upon him) said 'Save yourself from Hellfire even if it is by giving half a date in charity.'\", 'target'] = 0\n",
    "train.loc[train['text'] == \"Caution: breathing may be hazardous to your health.\", 'target'] = 1\n",
    "train.loc[train['text'] == \"I Pledge Allegiance To The P.O.P.E. And The Burning Buildings of Epic City. ??????\", 'target'] = 0\n",
    "train.loc[train['text'] == \"#Allah describes piling up #wealth thinking it would last #forever as the description of the people of #Hellfire in Surah Humaza. #Reflect\", 'target'] = 0\n",
    "train.loc[train['text'] == \"that horrible sinking feeling when youÛªve been at home on your phone for a while and you realise its been on 3G this whole time\", 'target'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [text, target]\n",
       "Index: []"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duplicate = train[train.duplicated('text',keep = False)]\n",
    "duplicate2 = train[train.duplicated(keep=False)]\n",
    "duplicate[~duplicate.isin(duplicate2)].dropna() #this will return empty dataframe now as we corrected all the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start Cleanong the Data as there are lots of hashtags and urls and mistakes in  the texts\n",
    "def remove_url(text):\n",
    "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url.sub(r'',text)\n",
    "\n",
    "def remove_punct(text):\n",
    "    table=str.maketrans('','',string.punctuation)\n",
    "    return text.translate(table)\n",
    "def remove_html(text):\n",
    "    html=re.compile(r'<.*?>')\n",
    "    return html.sub(r'',text)\n",
    "\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "def decontraction(text):\n",
    "    text = re.sub(r\"won\\'t\", \" will not\", text)\n",
    "    text = re.sub(r\"won\\'t've\", \" will not have\", text)\n",
    "    text = re.sub(r\"can\\'t\", \" can not\", text)\n",
    "    text = re.sub(r\"don\\'t\", \" do not\", text)\n",
    "    \n",
    "    text = re.sub(r\"can\\'t've\", \" can not have\", text)\n",
    "    text = re.sub(r\"ma\\'am\", \" madam\", text)\n",
    "    text = re.sub(r\"let\\'s\", \" let us\", text)\n",
    "    text = re.sub(r\"ain\\'t\", \" am not\", text)\n",
    "    text = re.sub(r\"shan\\'t\", \" shall not\", text)\n",
    "    text = re.sub(r\"sha\\n't\", \" shall not\", text)\n",
    "    text = re.sub(r\"o\\'clock\", \" of the clock\", text)\n",
    "    text = re.sub(r\"y\\'all\", \" you all\", text)\n",
    "\n",
    "    text = re.sub(r\"n\\'t\", \" not\", text)\n",
    "    text = re.sub(r\"n\\'t've\", \" not have\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'s\", \" is\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"\\'d've\", \" would have\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'ll've\", \" will have\", text)\n",
    "    text = re.sub(r\"\\'t\", \" not\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'m\", \" am\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    return text \n",
    "\n",
    "def seperate_alphanumeric(text):\n",
    "    words = text\n",
    "    words = re.findall(r\"[^\\W\\d_]+|\\d+\", words)\n",
    "    return \" \".join(words)\n",
    "\n",
    "def cont_rep_char(text):\n",
    "    tchr = text.group(0) \n",
    "    \n",
    "    if len(tchr) > 1:\n",
    "        return tchr[0:2] # take max of 2 consecutive letters\n",
    "\n",
    "def unique_char(rep, text):\n",
    "    substitute = re.sub(r'(\\w)\\1+', rep, text)\n",
    "    return substitute\n",
    "\n",
    "train['text']=train['text'].apply(lambda x : remove_url(x))\n",
    "train['text']=train['text'].apply(lambda x : remove_punct(x))\n",
    "train['text']=train['text'].apply(lambda x : remove_emoji(x))\n",
    "train['text']=train['text'].apply(lambda x : decontraction(x))\n",
    "train['text']=train['text'].apply(lambda x : seperate_alphanumeric(x))\n",
    "train['text']=train['text'].apply(lambda x : unique_char(cont_rep_char,x))\n",
    "\n",
    "test['text']=test['text'].apply(lambda x : remove_url(x))\n",
    "test['text']=test['text'].apply(lambda x : remove_punct(x))\n",
    "test['text']=test['text'].apply(lambda x : remove_emoji(x))\n",
    "test['text']=test['text'].apply(lambda x : decontraction(x))\n",
    "test['text']=test['text'].apply(lambda x : seperate_alphanumeric(x))\n",
    "test['text']=test['text'].apply(lambda x : unique_char(cont_rep_char,x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "155968"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X=train.append(test)\n",
    "corpus=create_corpus(X)\n",
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences=[]\n",
    "for i in range(len(train)):\n",
    "    train_sentences.append(train.iloc[i,0])\n",
    "test_sentences=[]\n",
    "for i in range(len(test)):\n",
    "    test_sentences.append(test.iloc[i,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "oov_tok = \"<OOV>\"\n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "max_length=120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "vocab_size=len(word_index)\n",
    "\n",
    "train_sequences = tokenizer.texts_to_sequences(train_sentences)\n",
    "train_padded = pad_sequences(train_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "test_sequences = tokenizer.texts_to_sequences(test_sentences)\n",
    "test_padded = pad_sequences(test_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = {};\n",
    "with open('glove.6B.100d.txt',encoding = \"utf-8\") as f:\n",
    "    for line in f:\n",
    "        values = line.split();\n",
    "        word = values[0];\n",
    "        coefs = np.asarray(values[1:], dtype='float32');\n",
    "        embeddings_index[word] = coefs;\n",
    "\n",
    "embeddings_matrix = np.zeros((vocab_size+1, embedding_dim));\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word);\n",
    "    if embedding_vector is not None:\n",
    "        embeddings_matrix[i] = embedding_vector;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_padded = np.array(train_padded)\n",
    "y=np.array(y)\n",
    "test_padded = np.array(test_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 120, 100)          2201400   \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 120, 64)           34048     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 120, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 116, 32)           10272     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 38, 32)            0         \n",
      "_________________________________________________________________\n",
      "bidirectional_4 (Bidirection (None, 38, 64)            16640     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 38, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 34, 32)            10272     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 11, 32)            0         \n",
      "_________________________________________________________________\n",
      "bidirectional_5 (Bidirection (None, 64)                16640     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 2,289,337\n",
      "Trainable params: 87,937\n",
      "Non-trainable params: 2,201,400\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size+1, embedding_dim, input_length=max_length,weights=[embeddings_matrix], trainable=False),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32,return_sequences=True)),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Conv1D(32, 5, activation='relu'),\n",
    "    tf.keras.layers.MaxPooling1D(pool_size=3),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32,return_sequences=True)),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Conv1D(32, 5, activation='relu'),\n",
    "    tf.keras.layers.MaxPooling1D(pool_size=3),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "238/238 [==============================] - 11s 47ms/step - loss: 0.3091 - accuracy: 0.8744\n",
      "Epoch 2/10\n",
      "238/238 [==============================] - 12s 52ms/step - loss: 0.2933 - accuracy: 0.8782\n",
      "Epoch 3/10\n",
      "238/238 [==============================] - 12s 52ms/step - loss: 0.2757 - accuracy: 0.8853\n",
      "Epoch 4/10\n",
      "238/238 [==============================] - 12s 52ms/step - loss: 0.2689 - accuracy: 0.8878\n",
      "Epoch 5/10\n",
      "238/238 [==============================] - 12s 52ms/step - loss: 0.2508 - accuracy: 0.8983\n",
      "Epoch 6/10\n",
      "238/238 [==============================] - 12s 52ms/step - loss: 0.2454 - accuracy: 0.8989\n",
      "Epoch 7/10\n",
      "238/238 [==============================] - 12s 52ms/step - loss: 0.2294 - accuracy: 0.9057\n",
      "Epoch 8/10\n",
      "238/238 [==============================] - 12s 50ms/step - loss: 0.2129 - accuracy: 0.9155\n",
      "Epoch 9/10\n",
      "238/238 [==============================] - 12s 51ms/step - loss: 0.2061 - accuracy: 0.9183\n",
      "Epoch 10/10\n",
      "238/238 [==============================] - 12s 51ms/step - loss: 0.1933 - accuracy: 0.9245\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x22361169a48>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_padded, y, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [1],\n",
       "       [1],\n",
       "       ...,\n",
       "       [1],\n",
       "       [0],\n",
       "       [0]], dtype=int16)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result=model.predict(test_padded)\n",
    "result=np.rint(result)\n",
    "result=result.astype('int16')\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "res=pd.read_csv('sample_submission.csv',index_col=None)  #Creating a submission file\n",
    "res['target']=result\n",
    "res.to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
